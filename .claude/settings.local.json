{
  "permissions": {
    "allow": [
      "Bash(\"llama_venv/Scripts/python.exe\" -m pip uninstall llama-cpp-python -y)",
      "Bash(\"llama_venv/Scripts/python.exe\" -m pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121)",
      "Bash(\"llama_venv/Scripts/python.exe\" -m pip install llama-cpp-python --force-reinstall --no-cache-dir --extra-index-url https://jllllll.github.io/llama-cpp-python-cuBLAS-wheels/AVX2/cu121)",
      "Bash(\"llama_venv/Scripts/python.exe\" llama_grammar_correct.py test_grammar_errors.txt json)",
      "Bash(\"llama_venv/Scripts/python.exe\" test_gpu.py)",
      "Bash(nvidia-smi:*)",
      "Bash(\"llama_venv/Scripts/python.exe\" --version)",
      "Bash(\"llama_venv/Scripts/python.exe\" -m pip install:*)",
      "Bash(\"llama_venv/Scripts/python.exe\" -m pip list)",
      "Bash(findstr:*)",
      "Bash(\"llama_venv/Scripts/python.exe\":*)",
      "Bash(cmake:*)",
      "Bash(py:*)",
      "Bash(where:*)",
      "Bash(\"llama_venv_gpu/Scripts/python.exe\" -m pip install --upgrade pip)",
      "Bash(\"llama_venv_gpu/Scripts/python.exe\" -m pip install:*)",
      "Bash(\"llama_venv_gpu/Scripts/python.exe\" test_gpu.py)",
      "Bash(\"llama_venv_gpu/Scripts/python.exe\" -m pip uninstall llama-cpp-python -y)",
      "Bash(\"llama_venv_gpu/Scripts/python.exe\" -m pip install llama-cpp-python --force-reinstall --no-cache-dir)",
      "Bash(set CMAKE_ARGS=-DGGML_CUDA=on)",
      "Bash(set:*)",
      "Bash(set CUDA_PATH=\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v13.0\")",
      "Bash(cmd /c install_cuda.bat)",
      "Bash(cmd /c:*)",
      "Bash(\"llama_venv_gpu/Scripts/python.exe\" -m pip:*)",
      "Bash(dir \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v13.0\\bin\\nvcc.exe\")",
      "Bash(\"llama_venv_gpu/Scripts/python.exe\" llama_grammar_correct.py:*)",
      "Bash(dir:*)",
      "Bash(\"llama_venv_gpu/Scripts/python.exe\":*)",
      "Bash(del \"models\\llama-3.1-8b-instruct-q4_k_m.gguf\")",
      "Read(//c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v13.0/bin/**)",
      "Bash(rm:*)",
      "Bash(mkdir:*)",
      "Bash(mv:*)",
      "Bash(cat:*)",
      "Bash(\"llama_venv/Scripts/python.exe\" -c \"from llama_cpp import Llama; print(Llama.__file__)\")",
      "Bash(\"llama_venv/Scripts/python.exe\" -m pip:*)",
      "Bash(cargo check:*)",
      "Bash(cargo clean:*)",
      "Bash(cargo build:*)",
      "Bash(tasklist:*)",
      "Bash(npm run tauri:build:*)",
      "Bash(git init:*)"
    ],
    "deny": [],
    "ask": []
  }
}
